---
title: "Predicting Hotel Booking Cancellation"
subtitle: "Final Report of Group 9 - Team Project #2"
author: "Lando Schwerdtfeger, Mitchell Fuchs, Nick Lo, Maria Gonzalez, Tomas Llopert"
date: "November 26th, 2025"
output:
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    highlight: tango
    theme: simplex
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache = FALSE)
```

<style>
#watermark-logo {
  position: fixed;
  bottom: 25px;
  left: 25px;
  width: 100px;
  opacity: 1;
  z-index: 9999;
}
</style>

<img id="watermark-logo" src="images/logo.png">

![](images/crowne.jpeg)

[Hotel Booking Reservation Dataset](https://www.kaggle.com/datasets/kundanbedmutha/hotel-booking-reservation)

## Step -1 - Explanation of Methodology

The project directory is organized to separate data, code, intermediate model objects, and the final written analysis. This layout follows a modular design that keeps the workflow organized, supports version control through GitHub, and makes the project easy to reproduce.

#### Components:

##### project.Rproj
The RStudio project file that ensures consistent paths and settings across different environments.

##### hotel_project.Rmd / hotel_project.html
The R Markdown file and the knitted HTML document that present the full analysis and results.

##### images/
Contains static images referenced in the write-up, such as diagrams or figures included for explanation. It does not store plots generated automatically during the analysis.

##### data/
Contains the raw dataset as well as the script used for initial data cleaning and splitting.

#####  models/
Stores the R scripts used to train, tune, and evaluate all first-level and stacked models. These files are sourced in the main Rmd.

##### RDS/
Holds serialized R objects (“R Data Serialization”). These files store trained models and precomputed outputs such as prediction probabilities, enabling the analysis to be rerun without retraining every model. This format also supports future extensions, such as loading models directly into an interactive Shiny application.


# Step 0 - Why?

We have been hired by IHG, a global Hotel and Resorts brand, to help the management of the City Hotel Ahmedabad. IHG has been facing the problem of revenue loss due to room cancellations. It is shown that around 40% of rooms are canceled after booking. Because cancellations are common, hotels usually “overbook” a little bit to avoid having empty rooms. IHG does not wish to overbook too aggressively since they would have to compensate the guests with a better room and compliments. On the other hand, if we play it too safe and don’t overbook at all, we leave expensive rooms empty. This prompts IHG to explore models that would allow them to overbook the rooms to minimize missed revenue. We aim to predict which reservations are likely to cancel before the guest shows up. 

We act as consultants, and management has provided us with a data set of 4,850 reservations with 27 variables plus the final binary outcome is_canceled, which is 1 if the guest cancelled and 0 if they actually stayed. We are trying to predict this outcome through the other variables that describe what the hotel already knows at booking time like lead time, length of previous stay, number of guests, room type, booking channel, and previous cancellations.
	
In this project our prediction task is to estimate, for each new booking, the probability that it will be canceled. We will build and compare several models including Logistic Regression, KNN (k-Nearest-Neighbours), ANN (Artificial Neural Network), Decision Tree, Random Forest, and Support Vector Machine. We aim to optimize precision since we would want to avoid false positives (predicting the guest to cancel but they actually show up) which would incur extra cost. We'll then build a stacked meta model (second layer decision tree) that leverages the predicted probabilities from the base models as inputs and produces a final cancellation probability which the hotel can use when setting its overbooking policy.

We will evaluate the differences between our baseline without a model, all individual models, a very conservative logistic regression model that prioritizes minimizing false positives, and the stacked model in classic statistical metrics and compute the expected net financial impact of model implementation. From this analysis, we are able to provide the management with a recommendation of which model to implement and how to do so. They will also be able to optimize room reservations to prevent revenue loss due to empty rooms and overbooking.

# Step 1, 2, 3 : Load, Clean & Split Data

```{r data}
source("data/data_cleaning_splitting.R")
```

##### Step 1 - Load Data
We began by loading the hotel booking dataset, which contains reservation records of multiple hotels, including booking characteristics, and whether each booking was eventually canceled.

##### Step 2 - Clean Data
As our scenario focuses on a single hotel, we filtered the data to include only reservations from the City Hotel in Ahmedabad, India.

We removed several fields that provided no useful predictive information (e.g. company, arrival_date_year) and post-event outcomes (i.e. assigned_room_type) and removed all remaining NA's.

We separated the target variable is_canceled from the predictors and converted every categorical feature into numerical form through one-hot encoding so that the models could interpret them correctly.

##### Step 3 - Split Data
We created a 60% training / 40% test split. We then applied min–max scaling using only the ranges computed from the training set and applied these parameters consistently to both subsets..

Finally, we recombined the scaled predictors with the cancellation outcome to form df_train and df_test. We also visualized the proportion of cancellations in the training data using a simple bar chart to get a sense of class balance.

# Step 4, 5, 6 : Create, Predict & Evaluate Models

## First-level Models

### A - Logistic Regression

We started by builing a logistic regression model, using stepwise to remove uninformative variables.

```{r logistic_regression_results}
source("models/LogisticRegression.R")
```

The backward stepwise selection, which removed variables one at a time to find the model with the lowest AIC, and over 8 steps we dropped variables like distribution channel, arrival date features, country, room type, market segment, meal type, and number of adults, ultimately reducing the AIC from 2590.027 to 2578.054. The reduction in AIC indicates that our final model is more efficient, retaining predictive power while eliminating redundant variables. It balances fit and simplicity, providing a solid baseline for further modeling and evaluation.

After finding our best model, we tested 99 different probability thresholds to determine when to classify a booking as canceled, and the threshold that maximized precision was 0.92 (but 0.99 resulted in less revenue lost), meaning we only predict cancellation when we're extremely confident. Our goal was to create a conservative baseline model in which we also had to accept that we'd miss many actual cancellations in order to avoid false positives. 

The baseline model that did nto allow overbooking when guests do show up only caught about 38% of actual cancellations for an overall accuracy of 73.09%.

### B - KNN

```{r knn_results}
source("models/KNN.R")
```

We implemented a K-Nearest Neighbors (KNN) model as our second first-level model. 
To optimize the latter, we tested a range of odd k-values from 3 to 39 to find the configuration that maximized AUC (the Area under the ROC-curve) for strong segmentation between canceled and non-canceled bookings.

The selected best k for KNN was 35, which lies between the two common heuristics discussed in class: $k = \sqrt{n/2} ≈ 31$ and $k = \sqrt{n} ≈ 44$, given that the training sample size $n = 1,940$. This confirms that our data-driven selection is consistent with classical guidance, balancing bias and variance in the model.

We then generated probability predictions for both training and test sets for later use in stacking. Using a 0.5 threshold to classify cancellations, KNN achieved an accuracy of 74.3%, sensitivity of 47.7%, and specificity of 94.4%. This indicates the model was conservative in predicting cancellations, reliably avoiding false positives, which helps reduce potential revenue loss.

The accompanying plot shows AUC as a function of k, highlighting the chosen optimal setting.

### C - ANN

![](images/ANN_hotel.png)

```{r ann_results}
source("models/ANN.R")
```

Due to expensive computations using ANN, we took a subset of the train data since the full dataset takes more than 20 minutes to knit. We played around with different neurons in two layers and ultimately applied 3 neurons for the first layer and 2 neurons for the second layer.

On the test set, the model achieved an accuracy of 74.3%, with a Kappa of 46.3%, sensitivity of 60.3%, and specificity of 84.9%. 

### D - SVM

We trained support vector machines using linear, radial, and sigmoid kernels and compared them in the following table:

```{r svm_results}
source("models/SVM.R")
```

The models using the linear and radial kernel emerged as candidates for the stacked model. Because the linear kernel had a slight edge in Accuracy, Kappa  and Sensitivity, we chose to feed this one to the meta-learner. 

The confusion matrix shows that the model captures a substantial portion of cancellations while maintaining a low false positive rate. The linear SVM reached a Kappa of 55.6% with an accuracy of 79.0%, sensitivity of 66.1%, and specificity of 88.7%. Based on these results, the linear kernel was selected as the best SVM configuration and its probabilities were saved for potential stacking in later analyses.

### E - Decision Tree

We trained two decision tree models to predict cancellations: a standard rpart model with default complexity parameters and a C5.0 model with 10 boosting trials. For the rpart model, we first built a baseline tree and then tuned the complexity parameter via cross-validation to minimize error. Probabilities from both models were generated for both training and test sets, which were later saved for potential stacking.

```{r dt_results}
source("models/DecisionTree.R")
```

The tuned rpart plot highlights which variables most strongly affect cancellation risk and provides a visual overview of how decisions are made across the dataset.

If the deposit type is refundable, the booking is very likely to be canceled. Previous cancellations, the online market segment, and the number of special requests also influence the likelihood of cancellation.

Both the C5.0 and the tuned rpart approach achieved similar accuracy, however the C5.0 model reached a higher AUC of 0.893, indicating superior discrimination between canceled and non-canceled bookings. Therefore, it will be used as a representative of the Decision Tree family in the stacked model.

The confusion matrix for C5.0 shows that it correctly identifies 90.0% of cancellations, with a specificity of 68.1%, reflecting a higher recall at the expense of some false positives.

### F - Random Forest


```{r rf_results}
source("models/RandomForest.R")
```

We optimized our Random Forest model over the number of trees (ntree) by evaluating performance metrics across a sequence from 100 to 1000 trees in steps of 50.

The AUC reached its maximum at 300 trees, which we selected as the final model. 
The corresponding confusion matrix confirms that the model balances capturing actual cancellations with minimizing false alarms. Probabilities from this model were also saved for potential use in stacked ensemble models.

This Random Forest achieved an overall accuracy of 82.0% and a Kappa of 0.627, indicating strong agreement beyond chance. Sensitivity was 72.6%, showing that most cancellations were correctly identified, while specificity was 89.1%, reflecting a low false positive rate.

## Evaluating first level Models

#### Summary for First-Level Models

We optimized the six base models over their respective parameters along statistical metrics. However, we are really interested to see how they perform in financial terms. We will shortly introduce our benefit-analysis approach, define a cost-matrix and compare the first-level models with this monetary metric.

#### Introducing Cost Matrix 

There are multiple ways to model the impact of our model to the business case. One could try to plainly maximize revenue, maximize the net benefit of a model compared to a baseline of no model in place, or compare the benefit of the models to an ideal scenario where no empty rooms and no overbooking happens, because every cancellation or not is anticipated.

We have chosen to move forward with the latter approach. This implies computing the average costs (or average loss) per booking (per row) induced by the model as a deviance from the perfect scenario.

We have estimated, assumed and defined the following components for our cost matrix.

###### Cost of True Negative (C_TN) = $\$0$
The customer does not cancel their booking and we expect this fact.
There will be a loss of $\$0$ since no room is left empty and no overbooking occurs.

###### Cost of True Positive (C_TP) = $\$0$
The customer cancels their booking, but we correctly anticipate this outcome
Because we have anticipated this outcome, we overbooked the customers room. Therefore, we incur a loss of $\$0$ since the room is occupied by another customer.

###### Cost of False Negative (C_FN) = $\$500$
The customer cancels their booking, however we expected him to arrive.
The Hotel incurs a loss of $\$500$ which is an average estimate of the revenue that would have been generated by a stay in an average room, with an average amount of people for an average amount of nights.

###### Cost of False Positive (C_FP) = $\$1200$
The customer does not cancel their booking, but we expected him to cancel.
Because we anticipated a cancellation, we have overbooked the customers room. Hence, the room is occupied by another customer, which is why we have to offer alternative accommodation for the customer we expected to cancel. Our cost assumption amounts to $\$1200$ and includes providing a replacement room suite at a sister hotel ($\$800$), discounts and hotel restaurant credit worth $\$200$ and accounts for possible trickle-down effects on revenue from bad-reviews ($\$200$).

We began by sourcing the stored probabilities for the test set from the RDS files of all the hyperparameter-optimized first-level models. Then, we defined the _evaluate_at_threshold_ function that computed confusion matrices for every cutoff determining the classification of a booking as a cancellation or not. This function also computed the average cost per booking. 
We then ran a threshold sweep to find the cutoff that minimized _AvgCost_ for every model, and additionally included a cost-computation of no model in place to serve as a baseline reference for comparison.

The following table displays the results of average costs and metrics from the optimal confusion matrices. For visualization, we also included a plot of the results.

```{r comp}
source("models/base_comparison.R")
```

It becomes clear that the Random Forest emerges as the model with the lowest average costs of $111 using a threshold of 0.66. 

Models with extremely high thresholds, such as the ANN at 0.98, flag only the riskiest bookings, producing high precision but missing many cancellations. Models with moderate thresholds like KNN or SVM flag more bookings, increasing sensitivity but also generating higher false positives and costs.

Given that compensation for overbooking is more than twice as costly as leaving a room empty, the final confusion matrix for the best model, Random Forest, confirms its effectiveness in balancing sensitivity and specificity while minimizing financial loss.
This evaluation demonstrates that a cost-aware threshold selection is essential, allowing the model to minimize average losses per reservation rather than merely focusing on traditional metrics like accuracy or AUC.

As the ANN-Model is evidently the worst first-level models and as it was only trained on 30% of the test set, we have decided against incorporating it into the stacked models, which we will pursue in the following.

## Stacked Models

Now, we aim to combine the strengths of the first-level models into a single second-level model that could learn patterns across the predicted probabilities of first-level models.
For this approach, we initially selected a decision tree as the meta-learner. This choice allowed us to not only combine predictions but also let the model learn which base learners perform best under different conditions rather than simply averaging them. We again applied our previously defined cost matrix so that the second-level model could directly optimize for the business objective of minimizing financial loss from cancellations.

For training and evaluating of this meta-model, we performed a secondary data-split of 50-50 on the original test portion of the data. This creates two equal halves so that 20% of the full dataset can be used for training the meta-learner and the remaining 20% for final evaluation. This resulted in 970 observations in each set. 

### Initial second-layer Decision Tree

```{r meta_dt}
source("models/metaDT.R")
```

This initial stacked decision tree ultimately learned a very simple rule: it relied exclusively on the random forest probabilities as its predictor. This result is intuitive because the random forest clearly outperformed the other first-level models in our earlier evaluations. It also suggests that, in this configuration, stacking does not add value beyond what the random forest already provides on its own.

### Additional second-layer models

Because of this, we extended our analysis by first lowering the meta-decision-tree's complexity parameter to induce more splits. Moreover, we tested two additional approaches for meta-learners: a meta-logistic-regression and a meta-random-forest, to see whether any of these could extract complementary information from the full set of base-model predictions.

These three layered approaches will be evaluated against our best first-level-model: base Random Forest. 
To be able to fairly compare the average costs and confusion matrix metrics, we made sure to evaluated our Random Forest probabilities on the same final test-set (20% of total data) that the meta-models are tested on. This mitigates the possibility that the test-set of the meta-models holds bookings that are dis-proportionally difficult to predict.
Technically, meta-models should have a slight edge now, as they were effectively trained on the initial $60%+20%=80%$ (training for first-level-models + training of meta-models), whereas we did not retrain the base random forest model (which now misses 20% of data across the train- & test-set).

The table below displays a comparison of our findings:

```{r stacked_lr}
source("models/StackedLR.R")
```

Firstly, the random forest’s average cost increased from $\$111$ (when evaluated on the full 40% test-set) to $\$123$ on the new meta-test split. This shift reflects the difference in test-set composition, validating the necessity of our mitigation for this fact.

As the decision tree plot shows, tuning the complexity parameter induced more splits. Still, the comparison table makes it clear that none of the meta-learners were able to outperform the original random forest model, producing around 4.5% higher costs.
This indicates that the base random forest already captures most of the predictive signal in the first-level models, leaving little for the stacked learners to improve upon.

Because the Random Forest Model slightly outperforms the stacked models even if evaluated on the same test-set, our analysis concludes in the recommendation of this model.

# Step 7: Implement Model

We have found that our models were fairly reliable at predicting Hotel booking cancellation. 
After ultimately deciding to use the base random forest model as our final recommendation, we can expect our predictions to have an accuracy close to 80%, as an evaluated of this model on the $n=1940$ test-set yielded 37 False Positives and 343 False Negatives.

The best estimates our modelling provides quantifies the revenue loss due to empty rooms and overbooking compensation to be around $\$111$ per booking on average.

This is a reduction of the average booking costs from $\$215$ to $\$111$, which corresponds to a relative costs decrease of 49%, compared to having no model in place.

Disregarding inflation, growth rate or seasonal effects, this would mean an additional revenue of $(\$215-\$111)*4850= \$504,400$ per fiscal year for The City Hotel in Ahmadabad. 

_This number was calculated by multiplying the cost difference with the number of reservations our data-set counted for 2024 for this specific hotel._

## Next Steps

We advise to use our random forest model operationally to predict cancellations for incoming bookings. We rely on an optimized probability threshold of 66% to decide whether a booking is likely to cancel, and for any case exceeding this threshold the system would set the room back back to available.
This allows the hotel to act on predictions in real time and reduce the number of empty rooms.

In practice, the model will need to be updated on a regular basis. Booking behavior changes over time, and the underlying data distribution may shift with seasons, customer segments, or pricing strategies. To remain reliable, the model should be retrained frequently on the newest available data and re-evaluated to ensure that the probability threshold and cost assumptions remain appropriate.

## Future Explorations

Our consulting project concludes with the present analysis. However, to convince the City Hotel to extend our services, we will shortly present several directions in which the Hotel could benefit from further development. These ideas span improvements to the economic framework, model architecture, data inputs, and operational integration.

#### 1. Refine and Contextualize the Cost Structure

The current cost matrix deliberately simplifies reality. It uses generalized values (for example, 500 dollars per lost booking) and assumes full refunds, no rebooking, and identical value across guests. Future work should replace these simplifications with reservation-specific financial information derived from the hotel’s systems. This includes pricing differences between room types, number of guests, multi-night stays, weekday and season effects, and channel-specific margins.
Costs should become dynamic: late cancellations are far more harmful than early ones; the probability of rebooking should be estimated iteratively; and acceptable overbooking levels should vary by time to arrival and seasonality.
A more realistic structure would also differentiate customer types, because loyal or high lifetime value guests should have higher misclassification costs due to long term brand risk. Similarly, operational stress on staff and reputational externalities from overbooking incidents could be quantified. These refinements would transform the cost matrix from an abstract scenario into a detailed financial instrument.

#### 2. Strengthen the Economic Context and Policy Simulations

Because cancellation prediction only generates value on high occupancy nights, the financial benefits of the model will be overstated without integrating demand forecasting. A joint system that predicts both cancellation risk and expected occupancy would ensure that overbooking decisions are used only when economically justified.
Beyond occupancy, the model could support policy simulations. These include exploring differentiated cancellation fees, refining refund policies, incorporating dynamic compensation schemes, and investigating partnerships or coordinated overbooking agreements with sister hotels. Policy experimentation would allow management to quantify the financial and experiential impact of strategic decisions before implementation.

#### 3. Expand Predictive Inputs and Data Scope

Our dataset includes 4,850 bookings from a single hotel and a single period. While rich in reservation-level predictors, it lacks important behavioral and contextual data. Future data collection could include travel purpose, loyalty program details, corporate identifiers, customer satisfaction measures, or survey-based cancellation reasons. External datasets such as local event calendars, weather forecasts, flight price trends, or demand indices could capture additional drivers.
Extending the dataset temporally (multiple years) and horizontally (additional properties within the hotel group) would mitigate overfitting to idiosyncratic patterns and improve generalization. A sensitivity analysis should also be conducted to test robustness across different cost assumptions, seasons and customer segments.

#### 4. Operational Deployment and Decision Support Tools

Finally, the model should be embedded into the hotel’s workflow. This includes continuous retraining on incoming data to avoid performance deterioration, automated monitoring of model drift, and periodic updating of the optimal threshold as financial assumptions change.
A Shiny dashboard could provide real-time predictions for each reservation, offer threshold adjustment with live cost feedback, visualize expected cancellations and overbooking risk, and give management a transparent interface for decision-making.
Operational tools could also estimate the actual resell probability of a cancelled room and track whether the model’s economic assumptions materialize in practice. This would close the feedback loop between predicted and realized outcomes.