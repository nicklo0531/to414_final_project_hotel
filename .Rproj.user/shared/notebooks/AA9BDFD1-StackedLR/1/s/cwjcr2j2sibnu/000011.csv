"0",""
"0","library(caret)"
"0","library(neuralnet)"
"0","library(randomForest)"
"0","library(e1071)"
"0","library(class)"
"0","library(rpart)"
"0","library(pROC)"
"0","library(dplyr)"
"0",""
"0","# Load train/test data from the shared preprocessed files"
"0","df_train <- read.csv(""data/hotel_train.csv"")"
"0","df_test  <- read.csv(""data/hotel_test.csv"")"
"0",""
"0","y_train <- df_train$is_canceled"
"0","y_test  <- df_test$is_canceled"
"0",""
"0","X_train <- df_train[, !(names(df_train) %in% ""is_canceled""), drop = FALSE]"
"0","X_test  <- df_test[,  !(names(df_test)  %in% ""is_canceled""), drop = FALSE]"
"0",""
"0","# 1. Base models: probabilities on train + test"
"0",""
"0","## Logistic regression (simple full model for stacking)"
"0","log_model <- glm(is_canceled ~ ., data = df_train, family = ""binomial"")"
"0","log_train_prob <- predict(log_model, df_train, type = ""response"")"
"0","log_test_prob  <- predict(log_model, df_test,  type = ""response"")"
"0",""
"0","## ANN (same idea as your ANN writeup, but also get train preds)"
"0","set.seed(12345)"
"0","df_train_small <- df_train[sample(nrow(df_train), size = 0.3 * nrow(df_train)), ]"
"0",""
"0","ann_model <- neuralnet("
"0","  is_canceled ~ .,"
"0","  data     = df_train_small,"
"0","  lifesign = ""minimal"","
"0","  stepmax  = 1e6,"
"0","  hidden   = c(3, 2)"
"0",")"
"0",""
"0","ann_train_prob <- as.numeric(predict(ann_model, df_train))"
"0","ann_test_prob  <- as.numeric(predict(ann_model, df_test))"
"0",""
"0","## KNN (tune k, then use best k for probs)"
"0",""
"0","library(class)"
"0",""
"0","# Set up X / y for KNN specifically"
"0","train_y <- factor(df_train$is_canceled, levels = c(0, 1))"
"0","test_y  <- factor(df_test$is_canceled,  levels = c(0, 1))"
"0",""
"0","train_x <- df_train[, names(df_train) != ""is_canceled"", drop = FALSE]"
"0","test_x  <- df_test[,  names(df_test)  != ""is_canceled"", drop = FALSE]"
"0",""
"0","# Sequence of k values to try"
"0","ks <- seq(3, 31, 2)"
"0","knn_tune_results <- data.frame("
"0","  k           = ks,"
"0","  Accuracy    = NA_real_,"
"0","  Specificity = NA_real_,"
"0","  Sensitivity = NA_real_"
"0",")"
"0",""
"0","# Tune KNN over k"
"0","for (i in seq_along(ks)) {"
"0","  pred_i <- knn(train = train_x, test = test_x, cl = train_y, k = ks[i])"
"0","  pred_i <- factor(pred_i, levels = c(""0"", ""1""))"
"0","  "
"0","  cm_i <- confusionMatrix(pred_i, test_y, positive = ""1"")"
"0","  knn_tune_results$Accuracy[i]    <- cm_i$overall[""Accuracy""]"
"0","  knn_tune_results$Specificity[i] <- cm_i$byClass[""Specificity""]"
"0","  knn_tune_results$Sensitivity[i] <- cm_i$byClass[""Sensitivity""]"
"0","}"
"0",""
"0","# Pick k with the best specificity (we care about avoiding false positives)"
"0","best_k_spec <- knn_tune_results$k[which.max(knn_tune_results$Specificity)]"
"0",""
"0","# Final KNN using best_k_spec, now with probabilities for stacking / cost matrix"
"0","knn_pred_test <- knn("
"0","  train = train_x,"
"0","  test  = test_x,"
"0","  cl    = train_y,"
"0","  k     = best_k_spec,"
"0","  prob  = TRUE"
"0",")"
"0","knn_prob_attr_test <- attr(knn_pred_test, ""prob"")"
"0","knn_test_prob <- ifelse(knn_pred_test == ""1"", knn_prob_attr_test, 1 - knn_prob_attr_test)"
"0",""
"0","# Train-side probabilities (KNN on train vs train)"
"0","knn_pred_train <- knn("
"0","  train = train_x,"
"0","  test  = train_x,"
"0","  cl    = train_y,"
"0","  k     = best_k_spec,"
"0","  prob  = TRUE"
"0",")"
"0","knn_prob_attr_train <- attr(knn_pred_train, ""prob"")"
"0","knn_train_prob <- ifelse(knn_pred_train == ""1"", knn_prob_attr_train, 1 - knn_prob_attr_train)"
"0",""
"0",""
"0","## Decision Tree (tuned rpart)"
"0","set.seed(12345)"
"0","df_train_dt <- df_train"
"0","df_test_dt  <- df_test"
"0","df_train_dt$is_canceled <- as.factor(df_train_dt$is_canceled)"
"0","df_test_dt$is_canceled  <- as.factor(df_test_dt$is_canceled)"
"0",""
"0","cp_grid <- expand.grid(cp = seq(0.001, 0.05, length.out = 10))"
"0","dtuned <- train("
"0","  is_canceled ~ ., data = df_train_dt,"
"0","  method     = ""rpart"","
"0","  trControl  = trainControl(method = ""cv"", number = 5),"
"0","  tuneGrid   = cp_grid"
"0",")"
"0","best_cp <- dtuned$bestTune$cp"
"0",""
"0","dt_model <- rpart("
"0","  is_canceled ~ .,"
"0","  data    = df_train_dt,"
"0","  method  = ""class"","
"0","  control = rpart.control(cp = best_cp)"
"0",")"
"0",""
"0","dt_train_prob <- predict(dt_model, df_train_dt, type = ""prob"")[, ""1""]"
"0","dt_test_prob  <- predict(dt_model, df_test_dt,  type = ""prob"")[, ""1""]"
"0",""
"0","## Random Forest (ntree = 800 from RF writeup)"
"0","set.seed(12345)"
"0","rf_model <- randomForest("
"0","  as.factor(is_canceled) ~ .,"
"0","  data  = df_train,"
"0","  ntree = 800"
"0",")"
"0",""
"0","rf_train_prob <- predict(rf_model, df_train, type = ""prob"")[, ""1""]"
"0","rf_test_prob  <- predict(rf_model, df_test,  type = ""prob"")[, ""1""]"
"0",""
"0","## SVM (linear kernel, chosen in SVM writeup)"
"0","set.seed(12345)"
"0","df_train_svm <- df_train"
"0","df_test_svm  <- df_test"
"0","df_train_svm$is_canceled <- as.factor(df_train_svm$is_canceled)"
"0","df_test_svm$is_canceled  <- as.factor(df_test_svm$is_canceled)"
"0",""
"0","svm_linear <- svm("
"0","  is_canceled ~ .,"
"0","  data        = df_train_svm,"
"0","  kernel      = ""linear"","
"0","  probability = TRUE"
"0",")"
"0",""
"0","svm_train_prob <- attr("
"0","  predict(svm_linear, df_train_svm, probability = TRUE),"
"0","  ""probabilities"""
"0",")[, ""1""]"
"0",""
"0","svm_test_prob <- attr("
"0","  predict(svm_linear, df_test_svm, probability = TRUE),"
"0","  ""probabilities"""
"0",")[, ""1""]"
"0",""
"0",""
"0","# 2. Stacked model: meta-logistic on base probs"
"0",""
"0","stack_train <- data.frame("
"0","  log         = log_train_prob,"
"0","  ann         = ann_train_prob,"
"0","  knn         = knn_train_prob,"
"0","  dt          = dt_train_prob,"
"0","  rf          = rf_train_prob,"
"0","  svm         = svm_train_prob,"
"0","  is_canceled = y_train"
"0",")"
"0",""
"0","stack_test <- data.frame("
"0","  log = log_test_prob,"
"0","  ann = ann_test_prob,"
"0","  knn = knn_test_prob,"
"0","  dt  = dt_test_prob,"
"0","  rf  = rf_test_prob,"
"0","  svm = svm_test_prob"
"0",")"
"0",""
"0","stack_model <- glm(is_canceled ~ ., data = stack_train, family = ""binomial"")"
"0","stack_test_prob <- predict(stack_model, stack_test, type = ""response"")"
"0",""
"0",""
"0","# 3. Cost matrix + threshold sweep for all models"
"0",""
"0","# Cost matrix: FPs are much more expensive than FNs"
"0","C_FP <- 1200  # false positive: predict cancel, guest shows (walk/overbook)"
"0","C_FN <- 500   # false negative: predict show, guest cancels (empty room)"
"0",""
"0","evaluate_at_threshold <- function(y_true, p_hat, threshold, C_FP, C_FN) {"
"0","  preds <- ifelse(p_hat >= threshold, 1, 0)"
"0","  "
"0","  TP <- sum(preds == 1 & y_true == 1)"
"0","  FP <- sum(preds == 1 & y_true == 0)"
"0","  FN <- sum(preds == 0 & y_true == 1)"
"0","  TN <- sum(preds == 0 & y_true == 0)"
"0","  "
"0","  cm <- confusionMatrix("
"0","    factor(preds, levels = c(0, 1)),"
"0","    factor(y_true, levels = c(0, 1)),"
"0","    positive = ""1"""
"0","  )"
"0","  "
"0","  cost     <- C_FP * FP + C_FN * FN"
"0","  avg_cost <- cost / length(y_true)"
"0","  "
"0","  data.frame("
"0","    Threshold   = threshold,"
"0","    Accuracy    = cm$overall[""Accuracy""],"
"0","    Kappa       = cm$overall[""Kappa""],"
"0","    Sensitivity = cm$byClass[""Sensitivity""],"
"0","    Precision   = cm$byClass[""Pos Pred Value""],"
"0","    Cost        = cost,"
"0","    AvgCost     = avg_cost"
"0","  )"
"0","}"
"0",""
"0","sweep_model <- function(y_true, p_hat, model_name,"
"0","                        C_FP, C_FN,"
"0","                        thresholds = seq(0.01, 0.99, by = 0.01)) {"
"0","  rows <- lapply(thresholds, function(t) {"
"0","    evaluate_at_threshold(y_true, p_hat, t, C_FP, C_FN)"
"0","  })"
"0","  out <- do.call(rbind, rows)"
"0","  out$Model <- model_name"
"0","  out"
"0","}"
"0",""
"0","results_all <- rbind("
"0","  sweep_model(y_test, log_test_prob,   ""Logistic"",             C_FP, C_FN),"
"0","  sweep_model(y_test, ann_test_prob,   ""ANN"",                  C_FP, C_FN),"
"0","  sweep_model(y_test, knn_test_prob,   ""KNN"",                  C_FP, C_FN),"
"0","  sweep_model(y_test, dt_test_prob,    ""Decision Tree"",        C_FP, C_FN),"
"0","  sweep_model(y_test, rf_test_prob,    ""Random Forest"",        C_FP, C_FN),"
"0","  sweep_model(y_test, svm_test_prob,   ""SVM (linear)"",         C_FP, C_FN),"
"0","  sweep_model(y_test, stack_test_prob, ""Stacked (meta-logit)"", C_FP, C_FN)"
"0",")"
"0",""
"0",""
"0","# 4. Original no-FP logistic + simple baseline"
"0",""
"0","## 4a. Original logistic regression (no false positives)"
"0","##     Source your LogisticRegression.R, which finds the precision-max threshold"
"0",""
"0","source(""models/LogisticRegression.R"")  # assumes it creates cancel_pred_m1, metric_summary, y_test"
"1","Initial AIC:"
"1"," "
"1","2582.024"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","1"
"1"," "
"1",": drop"
"1"," "
"1","arrival_date_week_number"
"1"," "
"1","AIC:"
"1"," "
"1","2582.024"
"1"," "
"1","->"
"1"," "
"1","2580.036"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","2"
"1"," "
"1",": drop"
"1"," "
"1","distribution_channelTA.TO"
"1"," "
"1","AIC:"
"1"," "
"1","2580.036"
"1"," "
"1","->"
"1"," "
"1","2578.054"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","3"
"1"," "
"1",": drop"
"1"," "
"1","arrival_date_day_of_month"
"1"," "
"1","AIC:"
"1"," "
"1","2578.054"
"1"," "
"1","->"
"1"," "
"1","2576.079"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","4"
"1"," "
"1",": drop"
"1"," "
"1","countryGBR"
"1"," "
"1","AIC:"
"1"," "
"1","2576.079"
"1"," "
"1","->"
"1"," "
"1","2574.27"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","5"
"1"," "
"1",": drop"
"1"," "
"1","market_segmentOffline.TA.TO"
"1"," "
"1","AIC:"
"1"," "
"1","2574.27"
"1"," "
"1","->"
"1"," "
"1","2572.509"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","6"
"1"," "
"1",": drop"
"1"," "
"1","mealHB"
"1"," "
"1","AIC:"
"1"," "
"1","2572.509"
"1"," "
"1","->"
"1"," "
"1","2572.125"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","7"
"1"," "
"1",": drop"
"1"," "
"1","adults"
"1"," "
"1","AIC:"
"1"," "
"1","2572.125"
"1"," "
"1","->"
"1"," "
"1","2571.89"
"1"," "
"1","
"
"1","Step"
"1"," "
"1","8"
"1"," "
"1",": drop"
"1"," "
"1","market_segmentDirect"
"1"," "
"1","AIC:"
"1"," "
"1","2571.89"
"1"," "
"1","->"
"1"," "
"1","2571.854"
"1"," "
"1","
"
"1","Final AIC:"
"1"," "
"1","2571.854"
"1"," "
"1","
"
"1","Best threshold by precision:"
"1"," "
"1","0.96"
"1"," "
"1","
"
"1","Confusion Matrix and Statistics

"
"1","          Reference
"
"1","Prediction"
"1","    0"
"1","    1"
"1","
         0"
"1"," 1104"
"1","  522"
"1","
         1"
"1","    0"
"1","  314"
"1","
"
"1",""
"1","                         "
"1","                 "
"1","
"
"1","               Accuracy :"
"1"," 0.7309          "
"1","
"
"1","                 95% CI :"
"1"," (0.7106, 0.7506)"
"1","
"
"1","    No Information Rate :"
"1"," 0.5691          "
"1","
"
"1","    P-Value [Acc > NIR] :"
"1"," < 2.2e-16       "
"1","
"
"1","                         "
"1","                 "
"1","
"
"1","                  Kappa :"
"1"," 0.4064          "
"1","
"
"1","                         "
"1","                 "
"1","
"
"1"," Mcnemar's Test P-Value :"
"1"," < 2.2e-16       "
"1","
"
"1","                         "
"1","                 "
"1","
"
"1","            Sensitivity :"
"1"," 0.3756          "
"1","
"
"1","            Specificity :"
"1"," 1.0000          "
"1","
"
"1","         Pos Pred Value :"
"1"," 1.0000          "
"1","
"
"1","         Neg Pred Value :"
"1"," 0.6790          "
"1","
"
"1","             Prevalence :"
"1"," 0.4309          "
"1","
"
"1","         Detection Rate :"
"1"," 0.1619          "
"1","
"
"1","   Detection Prevalence :"
"1"," 0.1619          "
"1","
"
"1","      Balanced Accuracy :"
"1"," 0.6878          "
"1","
"
"1","                         "
"1","                 "
"1","
"
"1","       'Positive' Class :"
"1"," 1               "
"1","
"
"1","                         "
"1","                 "
"1","
"
