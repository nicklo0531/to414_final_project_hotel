---
title: "Writeup"
output:
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    highlight: tango
    theme: lumen
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```

<style>
#watermark-logo {
  position: fixed;
  bottom: 25px;
  left: 25px;
  width: 100px;
  opacity: 1;
  z-index: 9999;
}
</style>

<img id="watermark-logo" src="images/logo.png">

![](images/crowne.jpeg)

[Hotel Booking Reservation Dataset](https://www.kaggle.com/datasets/kundanbedmutha/hotel-booking-reservation)

# Step 0 - Why?

We have been hired by IHG, a global Hotel and Resorts brand, to help the management of the City Hotel Ahmedabad. IHG has been facing the problem of revenue loss due to room cancellations. It is shown that around 40% of rooms are canceled after booking. Because cancellations are common, hotels usually “overbook” a little bit to avoid having empty rooms. IHG does not wish to overbook too aggressively since they would have to compensate the guests with a better room and compliments. On the other hand, if we play it too safe and don’t overbook at all, we leave expensive rooms empty. This prompts IHG to explore models that would allow them to overbook the rooms to minimize missed revenue. We aim to predict which reservations are likely to cancel before the guest shows up. 

We act as consultants, and management has provided us with a data set of 4,850 reservations with 27 variables plus the final binary outcome is_canceled, which is 1 if the guest cancelled and 0 if they actually stayed. We are trying to predict this outcome through the other variables that describe what the hotel already knows at booking time like lead time, length of previous stay, number of guests, room type, booking channel, and previous cancellations.
	
In this project our prediction task is to estimate, for each new booking, the probability that it will be canceled. We will build and compare several models including Logistic Regression, KNN (k-Nearest-Neighbours), ANN (Artificial Neural Network), Decision Tree, Random Forest, and Support Vector Machine. We aim to optimize precision since we would want to avoid false positives (predicting the guest to cancel but they actually show up) which would incur extra cost. We'll then build a stacked meta model (second layer decision tree) that leverages the predicted probabilities from the base models as inputs and produces a final cancellation probability which the hotel can use when setting its overbooking policy.

We will evaluate the differences between our baseline without a model, all individual models, a very conservative logistic regression model that prioritizes minimizing false positives, and the stacked model in classic statistical metrics and compute the expected net financial impact of model implementation. From this analysis, we are able to provide the management with a recommendation of which model to implement and how to do so. They will also be able to optimize room reservations to prevent revenue loss due to empty rooms and overbooking.

# Step 1 & 2 & 3 : Load, Clean, and Split Data

Refer to the *data_cleaning_splitting.Rmd* under the data folder.

```{r data}
source("data/data_cleaning_splitting.R")
```

# Step 4 & 5 & 6 : Create, Predict and Evaluate Models

## First-level Models

### A - Logistic Regression

We built a logistic regression model as our baseline, starting with all available predictor variables and an initial AIC of 2590.027. We used backward stepwise selection, which removed variables one at a time to find the model with the lowest AIC, and over 8 steps we dropped variables like distribution channel, arrival date features, country, room type, market segment, meal type, and number of adults, ultimately reducing the AIC to 2578.054. After finding our best model, we tested 99 different probability thresholds to determine when to classify a booking as canceled, and the threshold that maximized precision was 0.92 (but 0.99 resulted in less revenue lost), meaning we only predict cancellation when we're extremely confident. Our goal was to create a conservative baseline model in which we also had to accept that we'd miss many actual cancellations in order to avoid false positives. We only caught about 38% of actual cancellations for an overall accuracy of 73.09%. The monetary results from this conservative approach were promising though, because there was a significant reduction in lost revenue with this model.

Accuracy: 0.731
Sensitivity: 0.569
Specificity: 0.902
Precision: 0.80


```{r logistic_regression_results}
source("models/LogisticRegression.R")
```

### B - KNN

In order to optimize the KNN model, we tuned k by testing different values from 0 to 31. After obtaining the accuracy, sensitivity and specificity for each K, we have selected the one with the highest specificity. This is because false positives, predicting a cancellation when the guests actually show up, are very expensive. We want a model that can correctly identify non-cancellations.

Accuracy: 0.731
Sensitivity: 0.5691
Specificity: 0.9022
Precision: 0.80


```{r knn_results}
source("models/KNN.R")
```

In order to optimize the KNN model, we tuned k by testing different values from 0 to 31. After obtaining the accuracy, sensitivity and specificity for each K, we have selected the one with the highest specificity. This is because false positives, predicting a cancellation when the guests actually show up, are very expensive. We want a model that can correctly identify non-cancellations.

### C - ANN

```{r ann_results}
source("models/ANN.R")
```

Due to expensive computations using ANN, we took a subset of the train data since the full dataset takes more than 20 minutes to run. We played around with different neurons in two layers and ultimately applied 3 neurons for the first layer and 2 neurons for the second layer.

### D - SVM

```{r svm_results}
source("models/SVM.R")
```

We put three SVM kernels- linear, radial, and sigmoid- into trials to look for the optimal kernel. The linear kernel slightly outperformed radial, so we applied it for the final model.

Accuracy: 0.7897
Sensitivity: 0.6615
Specificity: 0.8868
Precision: 0.816

### E - Decision Tree

The decision tree is optimized using a trial of 10.
Accuracy: 0.8057
Sensitivity: 0.9004
Specificity: 0.6806
Precision: 0.838



```{r dt_results}
source("models/DecisionTree.R")
```


### F - Random Forest

Need to fix this -- idk how to add a matrix

```{r rf_results}
source("models/RandomForest.R")
```

To model cancellation behaviour, we trained a Random Forest classifier. We tuned the number of trees (ntree) by testing values from 100 to 2000 in steps of 100, using out-of-sample precision as the primary metric. Performance plateaued quickly, and 300 trees delivered the best balance of accuracy, sensitivity, and precision, so we selected ntree = 300 for the final model.

## Summary for First-Level Models
	
The six base models we used are optimized with their respective parameters and thresholds. Random forest is especially outperforming the other models, giving good combinations of the metrics. These base models provide a good foundation for our stacked model.

## Introducing Cost Matrix and Evaluating first level Models

```{r comp, cache = FALSE}
source("models/base_comparison.R")
```
As the ANN-Model is evidentely much worse than the other first-level models and as it was only trained on 30% of the test set, we have decided against incorporating it in the stacked models.

## Stacked Models

### Second-layer Decision Tree

```{r meta_dt, cache = FALSE}
source("models/metaDT.R")
```

### Second-layer Logistic Regression

```{r stacked_lr, cache = FALSE}
source("models/StackedLR.R")
```


Given that giving someone a room at a different hotel plus compensation is about twice as expensive as an empty $500 room, the Random Forest with threshold 0.55 is the cheapest policy: it creates the lowest average loss per reservation. The other models either send too many guests away to other hotels or leave too many rooms empty, so their average cost per booking is higher.

Looking at the table itself, each row is the “best” version of that model once we plug in our cost numbers. The Threshold column shows how high the predicted cancel-probability has to be before we treat a booking as a cancellation risk. Models with very high thresholds like the ANN at 0.99 are extremely picky and only flag the riskiest guests, which gives them huge Precision but lower Sensitivity because they miss a lot of actual cancellations. On the other side, the logistic model at 0.01 basically flags almost everyone, so its sensitivity is higher but its precision drops. Accuracy and Kappa summarize overall performance, but the main number we care about is AvgCost, which turns all those trade-offs into an estimated dollar loss per reservation. Random Forest ends up with the lowest AvgCost, while the stacked model and SVM are close behind, and the ANN is the most expensive because it leaves too many empty rooms.

with cost matrix (?)

without cost matrix (??)


# Step 7: Implement Model

## Next Steps

## Future Explorations


